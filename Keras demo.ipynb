{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a neural network in Keras with Tensorflow #\n",
    "\n",
    "### A walkthrough for the Machine Learning Club\n",
    "\n",
    "The goal is to demonstrate how to build a simple neural network using Keras, a popular open source neural network library.\n",
    "\n",
    "The demonstration will use the spam assassin corpus that was used in the Coursera / Stanford Machine Learning course that many members of the group have taken already (see week 7 assignment 'exercise 6'). In the coursera course, we trained a support vector machine (SVM) to classify spam. Here we will use a neural network instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup steps:\n",
    "- Create new environment and activate it\n",
    "- Install python and packages per requirements.txt\n",
    "- Run <code>conda install jupyter</code>\n",
    "- Use <code>conda install nb_conda</code> to get Jupyter to use the environment\n",
    "\n",
    "\n",
    "#### Warning: Keras and Tensorflow have a lot of dependencies - it will take a while to install them all.\n",
    "\n",
    "TODO requirements.txt\n",
    "\n",
    "- python\n",
    "- tensorflow\n",
    "- keras\n",
    "- matplotlib\n",
    "- numpy particular version 16.4.? to avoid TF warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library written for coursera exercise 6, providing functions for preprocessing emails (slightly modified for this demo)\n",
    "# and tell it where the vocab list is saved\n",
    "import utils\n",
    "spamAssassinPath = 'C:\\\\Users\\\\Jo\\\\Documents\\\\coursera\\\\ml-coursera-python-assignments\\\\Exercise6\\\\Data\\\\'\n",
    "utils.setVocabListPath(os.path.join(spamAssassinPath, 'vocab.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: feature extraction\n",
    "\n",
    "The data that we want to use is raw text from emails. To train a neural network, we need a fixed number of features for each training example. We therefore cannot use the text itself, but need to extract a set of features from each email. Per the Coursera course, we will use a vocabulary list to define a set of words that we are interested in. These are 'stem' words, with the endings removed (see examples below). We then parse each email in the corpus and record which of the vocab words are present in that email. All other info from the email is disregarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['access',\n",
       " 'accord',\n",
       " 'account',\n",
       " 'achiev',\n",
       " 'acquir',\n",
       " 'across',\n",
       " 'act',\n",
       " 'action',\n",
       " 'activ',\n",
       " 'actual',\n",
       " 'ad',\n",
       " 'adam']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the vocab list for info\n",
    "with open(os.path.join(spamAssassinPath, 'vocab.txt')) as fid:\n",
    "    vocab_list_contents = fid.read()\n",
    "vocab_list_contents.split()[21:45:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example feature extraction\n",
    "To illustrate how this works, here is an example email, along with the processed version (reduced to stem words), the matches in the vocab list, and the resulting feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Unprocessed email (string length 393):\n",
      "----------------\n",
      "> Anyone knows how much it costs to host a web portal ?\n",
      ">\n",
      "Well, it depends on how many visitors you're expecting.\n",
      "This can be anywhere from less than 10 bucks a month to a couple of $100. \n",
      "You should checkout http://www.rackspace.com/ or perhaps Amazon EC2 \n",
      "if youre running something big..\n",
      "\n",
      "To unsubscribe yourself from this mailing list, send an email to:\n",
      "groupname-unsubscribe@egroups.com\n",
      "\n",
      "\n",
      "----------------\n",
      "Processed email (63 word stems):\n",
      "----------------\n",
      "anyon know how much it cost to host a web portal well it depend on how mani visitor your expect thi can be anywher from less than number buck a month to a coupl of dollar number you should checkout httpaddr or perhap amazon ec number if your run someth big to unsubscrib yourself from thi mail list send an email to emailaddr\n",
      "\n",
      "----------------\n",
      "Matching word indices (55 matches in vocab list):\n",
      "----------------\n",
      "[85, 915, 793, 1076, 882, 369, 1698, 789, 1821, 1830, 882, 430, 1170, 793, 1001, 1894, 591, 1675, 237, 161, 88, 687, 944, 1662, 1119, 1061, 1698, 374, 1161, 476, 1119, 1892, 1509, 798, 1181, 1236, 511, 1119, 809, 1894, 1439, 1546, 180, 1698, 1757, 1895, 687, 1675, 991, 960, 1476, 70, 529, 1698, 530]\n",
      "\n",
      "----------------\n",
      "Feature vector (vector length 1899 with 45 non-zero entries:\n",
      "----------------\n",
      "...0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0...\n"
     ]
    }
   ],
   "source": [
    "# Extract Features from a sample email\n",
    "with open(os.path.join(spamAssassinPath, 'emailSample1.txt')) as fid:\n",
    "    file_contents = fid.read()\n",
    "print('----------------')\n",
    "print(f'Unprocessed email (string length {len(file_contents)}):')\n",
    "print('----------------')\n",
    "print(file_contents)\n",
    "\n",
    "processed_email, word_indices = utils.processEmail(file_contents, verbose=False)\n",
    "features = utils.emailFeatures(word_indices)\n",
    "\n",
    "print('----------------')\n",
    "print(f'Processed email ({len(processed_email)} word stems):')\n",
    "print('----------------')\n",
    "print(' '.join(processed_email))\n",
    "\n",
    "\n",
    "print('\\n----------------')\n",
    "print(f'Matching word indices ({len(word_indices)} matches in vocab list):')\n",
    "print('----------------')\n",
    "print(word_indices)\n",
    "\n",
    "# Print Stats\n",
    "print('\\n----------------')\n",
    "print(f'Feature vector (vector length {len(features)} with {sum(features>0)} non-zero entries:')\n",
    "print('----------------')\n",
    "print('...'+' '.join(features.astype(int).astype(str)[50:100])+'...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
